HOST=0.0.0.0
PORT=3001
NEGOTIATEAI_DATA_ROOT=./data
ALLOWED_ORIGINS=http://localhost:3000

# LLM provider: local | ollama | openai | anthropic
# - local / ollama: uses Ollama (or any OpenAI-compatible endpoint)
# - openai: uses OpenAI API
# - anthropic: uses Claude Messages API
LLM_PROVIDER=ollama

# Ollama / local (when LLM_PROVIDER=local or ollama)
# OLLAMA_* are preferred; VLLM_* kept as legacy fallbacks.
# Install: curl -fsSL https://ollama.com/install.sh | sh
# Pull model: ollama pull qwen3:30b-a3b
# NOTE: Use http://host.docker.internal:11434 when running inside Docker
#       (dev-up.sh rewrites localhost automatically).
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen3:30b-a3b
# Keep model loaded in GPU memory (0 = unload immediately, -1 = never unload)
OLLAMA_KEEP_ALIVE=30m

# Legacy vLLM vars (used when OLLAMA_* not set)
# NOTE: Use http://host.docker.internal:11434 when running inside Docker.
VLLM_BASE_URL=http://localhost:11434
VLLM_MODEL=qwen3:30b-a3b
VLLM_API_KEY=

# OpenAI
OPENAI_BASE_URL=https://api.openai.com
OPENAI_MODEL=gpt-4o-mini
OPENAI_API_KEY=
# Secret forwarded by Deepgram to /api/llm-proxy for local-LLM traffic.
LLM_PROXY_API_KEY=
LLM_MAX_TOKENS_VOICE=300
LLM_MAX_TOKENS_ANALYSIS=1024
# Max conversation turns sent to LLM for voice responses (keeps prompts short)
LLM_VOICE_CONTEXT_TURNS=10
# Timeout for LLM streaming requests (seconds)
LLM_STREAM_TIMEOUT_SECONDS=30

# Anthropic / Claude
ANTHROPIC_BASE_URL=https://api.anthropic.com
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
ANTHROPIC_API_KEY=

# Bright Data
BRIGHTDATA_API_TOKEN=

# Exa Search
EXA_SEARCH_ENABLED=false
EXA_API_KEY=
EXA_SEARCH_URL=https://api.exa.ai/search
EXA_SEARCH_RESULTS_LIMIT=5

# Upstash Redis / caching (optional)
UPSTASH_REDIS_URL=
# Use REDIS_URL for non-Upstash setups if preferred
REDIS_URL=
CACHE_ENABLED=false
CACHE_DEFAULT_TTL_SECONDS=300
CACHE_RESEARCH_TTL_SECONDS=300
CACHE_TASK_TTL_SECONDS=120
CACHE_ANALYSIS_TTL_SECONDS=300
CACHE_KEY_PREFIX=negotiateai

# Deepgram
DEEPGRAM_API_KEY=

# Twilio
TWILIO_ACCOUNT_SID=
TWILIO_AUTH_TOKEN=
TWILIO_PHONE_NUMBER=
TWILIO_WEBHOOK_HOST=https://your-public-url

# Logging
LOG_LEVEL=INFO
LOG_NOISY_EVENTS_EVERY_N=120
# Set 0 to suppress noisy, high-frequency ok-status events from stdout/file.
LOG_NOISY_ACTIONS=media_event,save_audio_chunk,media_mark_received
# Skip INFO logs for selected routes (comma-separated) unless status >= 400.
LOG_SKIP_REQUEST_PATHS=/health
LOG_PRETTY=true
# auto uses terminal detection, true forces colors, false disables.
LOG_COLOR=auto

# Optional Deepgram think request headers, e.g. {"Authorization":"Bearer <YOUR_OPENAI_KEY>"}
DEEPGRAM_VOICE_AGENT_THINK_ENDPOINT_HEADERS={}

# Deepgram Voice Agent (STT + TTS + orchestrator)
DEEPGRAM_VOICE_AGENT_ENABLED=true
DEEPGRAM_VOICE_AGENT_WS_URL=wss://agent.deepgram.com/v1/agent/converse
DEEPGRAM_VOICE_AGENT_LISTEN_MODEL=nova-3
DEEPGRAM_VOICE_AGENT_SPEAK_MODEL=aura-2-thalia-en
# Leave blank to inherit from LLM_PROVIDER; set explicitly to override.
DEEPGRAM_VOICE_AGENT_THINK_PROVIDER=
DEEPGRAM_VOICE_AGENT_THINK_MODEL=
DEEPGRAM_VOICE_AGENT_THINK_TEMPERATURE=0.7
DEEPGRAM_VOICE_AGENT_THINK_ENDPOINT_URL=
DEEPGRAM_VOICE_AGENT_THINK_ENDPOINT_HEADERS={}
